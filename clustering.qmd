---
title: "Clustering Collsion Data Using DBSCAN & Other Techniques"
author: "Nathan Kurtz-Enko"
format: pdf
message: false
warning: false
---

## Intro
It might be interesting to see how traffic collisions cluster through space and time. This could reveal hotspots or other patterns that we could analyze further. There are a number of techniques that we can use to cluster data. Here, we will also consider HDBSCAN.

## Libraries
In order to load, handle, and cluster our data, we will require the following packages.

```{r}
library(tidyverse)

library(dbscan)

library(knitr)

library(sf)
```

## Load & Clean Data
Now, let us load the data and confirm the shape and completeness. We can see that most of the columns are numeric, which the exception of `contributing_factor_vehicle_1` through `vehicle_type_code_2` (also the only columns with missing data).

```{r}
# Get data files
data_files <- list.files("./data/crashes/clean/", full.names = TRUE)

# Load data
data_raw <- list_rbind(map(data_files, read_csv))

# Look at the data
glimpse(data_raw)

# How many missing values are there?
colMeans(is.na(data_raw)) |>
 as_tibble(rownames = "variable") |>
 kable(caption = "Missing Data")

# Load the spatial extent of boroughs for future visualization
boroughs <- st_read("./data/nyc_boroughs.geojson")
```

Since there is missing data in a few columns (and those columns are character data types), we will simply relable those values as "missing". We could instead use a sophisticated approach of converting that categorical data into factors and then using a classification model to fill in the values. Or we could encod them as numbers and use a regression model to estimate them. But, given the size of the data (+2mil records) that would take a lot computational power (which my laptop does not have).

```{r}
data_clean <- data_raw |>
  # Get the lon/lat coords out of the geometry column
  mutate(geometry = str_sub(geometry, 3L, -2L)) |>
  separate(geometry, into = c("lon", "lat"), sep = ",") |>
  mutate(lon = as.numeric(lon), lat = as.numeric(lat)) |>
  # Classify data as missing
  mutate(across(where(is.character), \(x) coalesce(x, "missing"))) |>
  mutate(total_casualties = number_of_persons_injured + number_of_persons_killed)
  # filter(year(datetime_utc) == 2020)

```

HDBSCAN is available in R via the `dbscan` package. However, this implementation requires the data be a matrix rather than a data frame. So, we can take our "clean" data and convert that into a matrix. Additionally, we have a lot of data, and while it might be nice to cluster using everything, that is also computationally expensive. So we will grab a sample of points and use that for initial clustering. Then, we can use that model to predict clusters for the remainder of the data.

```{r}
set.seed(1234) # For reproducible results

data_sample <- data_clean |>
  # Encode categorical data
  # mutate(across(where(is.character), \(x) as.numeric(as_factor(x)))) |> 
  # Encode datetime data
  mutate(datetime_utc = as.numeric(datetime_utc)) |>
  select(lat, lon, datetime_utc) |>
  # Pull out a sample of the data
  sample_n(10000) |>
  # Convert to matrix for clustering
  as.matrix()
```

## Cluster Data
As previously mentioned, we can use HDBSCAN to cluster our data. The convenience of the HDBSCAN algorithm is that it does not assume a particular number of clusters or shape. So, it is more flexible at identifying structure in the data than K-Means and other techniques.

```{r}
# Min points is recommended as # variables + 1
min_points <- ncol(data_clean) + 1

# Create a clustering model
cluster_hdb <- hdbscan(data_sample, minPts = min_points)
```

## Predict Clusters
Now we can use our models to classify the clusters for the remainder of the data.

```{r}
# Define new data over which to predict clusters
data_new <- data_clean |>
  # Encode categorical data
  # mutate(across(where(is.character), \(x) as.numeric(as_factor(x)))) |> 
  # Encode datetime data
  mutate(datetime_utc = as.numeric(datetime_utc)) |>
  select(lon, lat, datetime_utc) |>
  # Convert to matrix
  as.matrix()

# Get the clusters using HDBSCAN
data_clean$hdb_id <- predict(cluster_hdb, data = data_sample, newdata = data_new)
```

## Visualize Results
We can visualize the results of this clustering to see how the different clusters appear spatially, and we can look at how they appear temporally, too. This should give an idea of any patterns that might exist. We will start off by looking at the K-Means clusters.

```{r}
# Split the data based on HDBSCAN clusters
split_hdb <- data_clean |>
  split(~as_factor(hdb_id)) |>
  map(\(x) st_as_sf(x, coords = c("lon", "lat"), crs = 4326)) 

# Geographically visualize HDBSCAN clusters
split_hdb |>
  walk(\(x) {
    s <- st_as_sf(st_sample(x, 1000), crs = 4326)
    plt <- ggplot() +
      geom_sf(data = boroughs) +
      geom_sf(data = s, color = "red") +
      theme_minimal() +
      labs(title = "HDBSCAN Cluster Collision Locations")
    plot(plt)
  })

# Temporally visualize K-Means clusters
map(split_hdb, \(x) range(x$datetime_utc))
```

## Analysis
The clusters seem evenly spread out across New York City. In the case of HDBSCAN, the clusters appear to pertain to seasons for specific years. This might suggest distinct difference between years and seasons, which we can evalutae with a Kolmogorov-Smirnov test. it might be worthwhile to drill down into the data based on time and analysis specific ranges.

```{r}
ks.test(
  split_hdb[[1]]$total_casualties,
  split_hdb[[2]]$total_casualties
)
```

Let us look at some time series data about the collisions to confirm this futher. We can see that there are clear seasonal patterns in at least the `number_of_persons_injured` variable (this is likely the case in other ones).

```{r}
walk(
  1:length(split_hdb),
  \(x) {
    plt <- split_hdb[[x]] |>
      mutate(date = as.Date(datetime_utc)) |>
      group_by(date) |>
      summarize(avg_cas = mean(total_casualties)) |>
      ungroup() |>
      ggplot() +
      geom_line(aes(x = date, y = avg_cas))
    plot(plt)
  }
)
```

## Save Results
We can save this result so that we can reference it later.

```{r}
write_rds(split_hdb, "./data/clusters.rds")
```